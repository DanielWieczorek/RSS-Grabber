Deeplearning4j OOM Exception Encountered for MultiLayerNetwork
Timestamp:                              2018-12-25 22:21:24.584
Thread ID                               30
Thread Name                             pool-3-thread-1


Stack Trace:
java.lang.OutOfMemoryError: Failed to allocate memory within limits: totalBytes (74605K + 7205M) > maxBytes (4068M)
	at org.bytedeco.javacpp.Pointer.deallocator(Pointer.java:580)
	at org.deeplearning4j.nn.layers.BaseCudnnHelper$DataCache.<init>(BaseCudnnHelper.java:121)
	at org.deeplearning4j.nn.layers.recurrent.CudnnLSTMHelper.activate(CudnnLSTMHelper.java:509)
	at org.deeplearning4j.nn.layers.recurrent.LSTMHelpers.activateHelper(LSTMHelpers.java:205)
	at org.deeplearning4j.nn.layers.recurrent.LSTM.activateHelper(LSTM.java:163)
	at org.deeplearning4j.nn.layers.recurrent.LSTM.activate(LSTM.java:135)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.ffToLayerActivationsInWs(MultiLayerNetwork.java:1057)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2629)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2587)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:160)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:63)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:52)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fitHelper(MultiLayerNetwork.java:1602)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1521)
	at de.wieczorek.nn.AbstractNeuralNetworkTrainer.train(AbstractNeuralNetworkTrainer.java:54)
	at de.wieczorek.rss.advisor.business.TradingNeuralNetworkTrainer$Proxy$_$$_WeldClientProxy.train(Unknown Source)
	at de.wieczorek.rss.advisor.business.TrainingTimer.run(TrainingTimer.java:47)
	at de.wieczorek.rss.advisor.business.TrainingTimer$Proxy$_$$_WeldClientProxy.run(Unknown Source)
	at de.wieczorek.rss.core.timer.RecurrentTaskRunner.run(RecurrentTaskRunner.java:42)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:514)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:299)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
	at java.base/java.lang.Thread.run(Thread.java:844)


========== Memory Information ==========
----- Version Information -----
Deeplearning4j Version                  1.0.0-beta3
Deeplearning4j CUDA                     deeplearning4j-cuda-9.2

----- System Information -----
Operating System                        Microsoft Windows 10
CPU                                     Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz
CPU Cores - Physical                    4
CPU Cores - Logical                     8
Total System Memory                       15.89 GB (17061289984)
Number of GPUs Detected                 1
  Name                           CC                Total Memory              Used Memory              Free Memory
  GeForce GTX 1080 Ti            6.1        11 GB (11811160064)     3.30 GB (3539651789)     7.70 GB (8271508275)

----- ND4J Environment Information -----
Data Type                               FLOAT
blas.vendor                             CUBLAS
os                                      Windows 10
backend                                 CUDA

----- Memory Configuration -----
JVM Memory: XMX                            3.97 GB (4265607168)
JVM Memory: current                         446 MB (467664896)
JavaCPP Memory: Max Bytes                  3.97 GB (4265607168)
JavaCPP Memory: Max Physical               7.95 GB (8531214336)
JavaCPP Memory: Current Bytes             72.86 MB (76396097)
JavaCPP Memory: Current Physical           2.48 GB (2658775040)
Periodic GC Enabled                     true
Periodic GC Frequency                   100 ms

----- Workspace Information -----
Workspaces: # for current thread        2
Current thread workspaces:
  Name                      State       Size                          # Cycles            
  WS_LAYER_WORKING_MEM      CLOSED             0 B                    1                   
  WS_ALL_LAYERS_ACT         CLOSED       378.26 MB (396636979)        1                   
Workspaces total size                    378.26 MB (396636979)
Helper Workspaces
  CUDNN_WORKSPACE                            7.52 MB (7882752)

----- Network Information -----
Network # Parameters                    218881
Parameter Memory                         855.00 KB (875524)
Parameter Gradients Memory              <not allocated>
Updater Number of Elements              437762
Updater Memory                             1.67 MB (1751048)
Updater Classes:
  org.nd4j.linalg.learning.AdamUpdater
Params + Gradient + Updater Memory         1.67 MB (1751048)
Iteration Count                         1
Epoch Count                             0
Backprop Type                           Standard
Workspace Mode: Training                ENABLED
Workspace Mode: Inference               ENABLED
Number of Layers                        4
Layer Counts
  DenseLayer                              1
  LSTM                                    2
  RnnOutputLayer                          1
Layer Parameter Breakdown
  Idx Name                 Layer Type           Layer # Parameters   Layer Parameter Memory
  0   layer0               LSTM                 70656                    276 KB (282624) 
  1   layer1               LSTM                 131584                   514 KB (526336) 
  2   layer2               DenseLayer           16512                  64.50 KB (66048)  
  3   layer3               RnnOutputLayer       129                       516 B          

----- Layer Helpers - Memory Use -----
Total Helper Count                      0
Helper Count w/ Memory                  0
Total Helper Persistent Memory Use             0 B

----- Network Activations: Inferred Activation Shapes -----
Current Minibatch Size                  256
Input Shape                             [256, 9, 1441]
Idx Name                 Layer Type           Activations Type                           Activations Shape    # Elements   Memory      
0   layer0               LSTM                 InputTypeRecurrent(128,timeSeriesLength=1441) [256, 128, 1441]     47218688      180.13 MB (188874752)
1   layer1               LSTM                 InputTypeRecurrent(128,timeSeriesLength=1441) [256, 128, 1441]     47218688      180.13 MB (188874752)
2   layer2               DenseLayer           InputTypeFeedForward(128)                  [256, 128]           32768            128 KB (131072)
3   layer3               RnnOutputLayer       InputTypeRecurrent(1)                      [256, 1, -1]         -256              -1 KB  
Total Activations Memory                 360.37 MB (377879552)
Total Activations Memory (per ex)          1.41 MB (1476092)
Total Activation Gradient Mem.           373.04 MB (391160832)
Total Activation Gradient Mem. (per ex)    1.46 MB (1527972)

----- Network Training Listeners -----
Number of Listeners                     1
Listener 0                              org.deeplearning4j.optimize.listeners.PerformanceListener@1106adf4
